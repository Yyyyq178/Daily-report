import arxiv
import google.generativeai as genai
import requests
import os
import datetime
import time
import re

# =================é…ç½®åŒºåŸŸ=================
# API é…ç½®
GENAI_API_KEY = os.environ.get("GEMINI_API_KEY")
genai.configure(api_key=GENAI_API_KEY)

# æ¨¡å‹é€‰æ‹© (å‡è®¾ 2026 å¹´ç¯å¢ƒï¼Œå¦‚æŠ¥é”™è¯·å›é€€åˆ° gemini-1.5-pro)
MODEL_FAST = 'gemini-3-flash-preview' # ç”¨äºå¿«é€Ÿè¯„åˆ†
MODEL_DEEP = 'gemini-2.5-pro'       # ç”¨äºæ·±åº¦åˆ†æ

# æ ¸å¿ƒå…³é”®è¯ (å‘½ä¸­è¿™äº›è¯çš„è®ºæ–‡å°†ä¼˜å…ˆå¤„ç†)
CORE_KEYWORDS = [
    "Image Restoration", "Super-Resolution", "Denoising", "Deblurring",
    "Masked Autoregressive", "MAR", "Diffusion Model", "Generative Prior",
    "High-Fidelity", "Perceptual Quality"
]

# å¹¿æ³›å…³é”®è¯ (ç”¨äºä¿ç•™å€™é€‰)
BROAD_KEYWORDS = [
    "Computer Vision", "Generative", "Transformer", "Gaussian Splatting", 
    "NeRF", "3D Generation", "Video Synthesis", "Multimodal"
]

# æ’é™¤å…³é”®è¯ (è¿‡æ»¤æ— å…³é¢†åŸŸ)
EXCLUDE_KEYWORDS = ["Medical", "MRI", "CT Scan"]

# =================æ•°æ®ç»“æ„=================
class Paper:
    def __init__(self, arxiv_id, title, summary, url, source="arXiv"):
        self.id = arxiv_id
        self.title = title.replace('\n', ' ')
        self.summary = summary.replace('\n', ' ')
        self.url = url
        self.source = source # "HuggingFace" or "arXiv"
        self.score = 0
        self.reasoning = ""
        self.analysis = ""

# =================æŠ“å–æ¨¡å—=================
def get_huggingface_papers():
    """è·å– HF Daily Papers (é«˜è´¨é‡æº)"""
    print("æ­£åœ¨æŠ“å– Hugging Face Daily Papers...")
    papers = {}
    try:
        # è¿™æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿæ¥å£ï¼Œå®é™…ä¸­ HF æ¯æ—¥è®ºæ–‡é€šå¸¸å¯ä»¥é€šè¿‡ API æˆ–ç½‘é¡µè§£æè·å–
        # è¿™é‡Œä¸ºäº†ç¨³å®šæ€§ï¼Œæˆ‘ä»¬æŠ“å– HF çƒ­é—¨æ¦œå•å¯¹åº”çš„ arXiv é“¾æ¥
        url = "https://huggingface.co/api/daily_papers"
        response = requests.get(url)
        if response.status_code == 200:
            data = response.json()
            # å‡è®¾è¿”å›ç»“æ„åŒ…å« paper åˆ—è¡¨
            for item in data[:10]: # åªå–å‰10çƒ­åº¦
                aid = item['paper']['id'] # é€šå¸¸æ˜¯ arxiv id
                papers[aid] = "HuggingFace Hot"
    except Exception as e:
        print(f"HF æŠ“å–å¤±è´¥ (éè‡´å‘½é”™è¯¯): {e}")
    return papers

def fetch_papers_data(hf_ids):
    """ä¸»æŠ“å–é€»è¾‘ï¼šåˆå¹¶ HF å’Œ arXiv æ•°æ®"""
    client = arxiv.Client()
    
    # [ä¿®æ”¹ç‚¹ 1]ï¼šå‡å°‘æ‰«æç¯‡æ•°ï¼Œä» 80 æ”¹ä¸º 40
    print("æ­£åœ¨æœç´¢ arXiv æœ€æ–°è®ºæ–‡ (Max: 40)...")
    search_arxiv = arxiv.Search(
        query="cat:cs.CV",
        max_results=40, # é™ä½è´Ÿè½½ï¼Œåªçœ‹æœ€æ–°çš„40ç¯‡
        sort_by=arxiv.SortCriterion.SubmittedDate
    )
    
    results = []
    seen_ids = set()

    # å¤„ç† arXiv ç»“æœ
    for result in client.results(search_arxiv):
        aid = result.get_short_id().split('v')[0]
        seen_ids.add(aid)
        
        # åˆ¤å®šæ¥æº
        source = "arXiv Latest"
        if aid in hf_ids:
            source = "ğŸ”¥ HuggingFace Hot" # åªè¦åœ¨ HF æ¦œå•ä¸Šï¼Œæ ‡è®°ä¸ºçƒ­ç‚¹
        
        p = Paper(aid, result.title, result.summary, result.entry_id, source)
        results.append(p)

    # 2. å¦‚æœ HF é‡Œçš„ ID æ²¡åœ¨ arXiv æœ€æ–°åˆ—è¡¨é‡Œï¼ˆå¯èƒ½æ˜¯å‡ å¤©å‰çš„çƒ­ç‚¹ï¼‰ï¼Œéœ€è¦è¡¥å……æŠ“å–
    missing_ids = [hid for hid in hf_ids if hid not in seen_ids]
    if missing_ids:
        print(f"è¡¥å……æŠ“å– {len(missing_ids)} ç¯‡ HF çƒ­é—¨è®ºæ–‡...")
        search_missing = arxiv.Search(id_list=missing_ids)
        for result in client.results(search_missing):
            aid = result.get_short_id().split('v')[0]
            p = Paper(aid, result.title, result.summary, result.entry_id, "ğŸ”¥ HuggingFace Hot")
            results.append(p)
            
    return results

# =================AI åˆ†ææ¨¡å—=================
def filter_and_score(papers):
    """
    ç¬¬ä¸€å±‚ï¼šPython å…³é”®è¯ç¡¬è¿‡æ»¤
    ç¬¬äºŒå±‚ï¼šGemini Flash å¿«é€Ÿè¯„åˆ†
    """
    candidates = []
    
    # 1. ç¡¬è¿‡æ»¤
    for p in papers:
        text = (p.title + p.summary).lower()
        if any(ex.lower() in text for ex in EXCLUDE_KEYWORDS):
            continue
        
        # è‡³å°‘å‘½ä¸­ä¸€ä¸ªå¹¿æ³›å…³é”®è¯ï¼Œæˆ–è€…æ¥è‡ª HF çƒ­æ¦œ
        if any(k.lower() in text for k in (CORE_KEYWORDS + BROAD_KEYWORDS)) or "Hot" in p.source:
            candidates.append(p)
            
    print(f"åˆç­›é€šè¿‡: {len(candidates)} ç¯‡ï¼Œå¼€å§‹ AI è¯„åˆ†...")
    
    # 2. AI è¯„åˆ†
    model = genai.GenerativeModel(MODEL_FAST)
    
    scored_papers = []
    for i, p in enumerate(candidates):
        # ç®€å•çš„è¿›åº¦æç¤º
        print(f"æ­£åœ¨è¯„åˆ† ({i+1}/{len(candidates)}): {p.title[:30]}...")

        # å¦‚æœæ ‡é¢˜åŒ…å«æ ¸å¿ƒå…³é”®è¯ï¼Œç›´æ¥åŠ åˆ†
        base_priority = "High" if any(k.lower() in p.title.lower() for k in CORE_KEYWORDS) else "Normal"
        
        prompt = f"""
        Role: CV Research Assistant.
        Task: Rate relevance (1-10) for a researcher focusing on: Image Restoration, MAR, Super-Resolution.
        Input:
        Title: {p.title}
        Abstract: {p.summary}
        
        Output format strictly: Score | One-sentence reason
        Example: 8 | Proposes a novel MAR variant for deblurring.
        """
        try:
            response = model.generate_content(prompt)
            text = response.text.strip()
            score_str = text.split('|')[0].strip()
            
            # è§£æåˆ†æ•°
            found_scores = re.findall(r"\d+", score_str)
            if found_scores:
                p.score = int(float(found_scores[0]))
            else:
                p.score = 5 # é»˜è®¤åˆ†
            
            p.reasoning = text.split('|')[1].strip() if '|' in text else text
            
            # æ ¸å¿ƒé¢†åŸŸè®ºæ–‡å¼ºè¡Œææƒ
            if base_priority == "High" and p.score < 7:
                p.score = 7 
            
            if p.score > 6: # åªä¿ç•™åŠæ ¼ä»¥ä¸Šçš„
                scored_papers.append(p)

        except Exception as e:
            print(f"è¯„åˆ†å¤±è´¥: {e}")
            time.sleep(4) # å‡ºé”™ä¹Ÿè¦ç­‰å¾…ï¼Œé˜²æ­¢æ­»å¾ªç¯è¯·æ±‚
            continue
        # [ä¿®æ”¹ç‚¹ 2]ï¼šå¢åŠ é—´éš”æ—¶é—´
        # Flash å…è´¹ç‰ˆé™åˆ¶çº¦ 15 RPM (æ¯åˆ†é’Ÿ15æ¬¡)ï¼Œå³ 4ç§’/æ¬¡
        time.sleep(60) 
    # æŒ‰åˆ†æ•°é™åºæ’åˆ—
    scored_papers.sort(key=lambda x: x.score, reverse=True)
    return scored_papers

def deep_analyze_paper(paper):
    """ä½¿ç”¨ Pro æ¨¡å‹è¿›è¡Œæ·±åº¦å®¡ç¨¿"""
    model = genai.GenerativeModel(MODEL_DEEP)
    
    prompt = f"""
    You are an expert reviewer for ECCV/CVPR.
    Analyze the following paper strictly in CHINESE (Markdown).
    
    Target Audience: A researcher working on **Image Restoration** and **Masked Autoregressive (MAR)** models.
    
    Paper:
    Title: {paper.title}
    Abstract: {paper.summary}
    
    Please provide:
    1. **æ ¸å¿ƒåˆ›æ–°ç‚¹ (The "Hook")**: What is strictly new? (1-2 bullet points)
    2. **æ–¹æ³•è®ºæ‹†è§£ (Methodology)**: How does it work? 
       - If it mentions MAR or Transformers, compare it with standard approaches.
    3. **æ½œåœ¨ç¼ºé™·/å±€é™ (Critical Review)**: As a reviewer, what would you challenge? (e.g., complexity, lack of specific baselines)
    4. **å¯¹æˆ‘çš„å¯å‘**: How can this apply to Image Restoration tasks?
    
    Output strictly in Markdown. No preamble.
    """
    
    try:
        response = model.generate_content(prompt)
        return response.text
    except Exception as e:
        return f"åˆ†æç”Ÿæˆå¤±è´¥: {e}"

# =================ä¸»ç¨‹åº=================
def main():
    # 1. è·å– ID åˆ—è¡¨
    hf_ids = get_huggingface_papers()
    
    # 2. æŠ“å–å…¨æ–‡æ•°æ®
    all_papers = fetch_papers_data(hf_ids)
    print(f"å…±æŠ“å–åŸå§‹è®ºæ–‡: {len(all_papers)} ç¯‡")
    
    if not all_papers:
        print("æœªæŠ“å–åˆ°è®ºæ–‡ï¼Œç¨‹åºç»“æŸã€‚")
        return

    # 3. ç­›é€‰ä¸è¯„åˆ†
    top_papers = filter_and_score(all_papers)
    print(f"æœ€ç»ˆå…¥é€‰ç²¾è¯»: {len(top_papers)} ç¯‡")
    
    # 4. ç”ŸæˆæŠ¥å‘Š
    today = datetime.date.today().strftime("%Y-%m-%d")
    md_content = f"# ğŸš€ CV è®ºæ–‡æ—¥æŠ¥ | {today}\n\n"
    md_content += f"> ğŸ¤– ä»Šæ—¥åŠ¨æ€ï¼šæ‰«æ {len(all_papers)} ç¯‡ï¼Œç²¾é€‰ {min(5, len(top_papers))} ç¯‡æ·±åº¦è§£è¯»ã€‚\n\n"
    
    # ç›®å½•éƒ¨åˆ†
    md_content += "## ğŸ“‹ ç›®å½• (Quick View)\n"
    for p in top_papers[:10]:
        icon = "ğŸ”¥" if "Hot" in p.source else "ğŸ“„"
        md_content += f"- **{p.score}åˆ†** {icon} [{p.title}]({p.url}) - *{p.reasoning}*\n"
    md_content += "\n---\n"
    
    # æ·±åº¦åˆ†æéƒ¨åˆ† (åªå–å‰ 5 ç¯‡ï¼Œä¿æŠ¤ Pro é¢åº¦)
    md_content += "## ğŸ§  æ·±åº¦è§£è¯» (Deep Dive)\n"
    
    deep_dive_count = min(5, len(top_papers))
    
    for i, p in enumerate(top_papers[:deep_dive_count]):
        print(f"æ­£åœ¨æ·±åº¦åˆ†æç¬¬ {i+1}/{deep_dive_count} ç¯‡: {p.title}...")
        analysis = deep_analyze_paper(p)
        
        md_content += f"### {i+1}. {p.title}\n"
        md_content += f"**æ¥æº**: {p.source} | **è¯„åˆ†**: {p.score}/10 | [Paper Link]({p.url})\n\n"
        md_content += f"{analysis}\n\n"
        md_content += "---\n"
        
        # [ä¿®æ”¹ç‚¹ 3]ï¼šå¢åŠ æ·±è¯»é—´éš”
        # Pro æ¨¡å‹å…è´¹ç‰ˆé€šå¸¸é™åˆ¶ 2 RPM (æ¯åˆ†é’Ÿ2æ¬¡)ï¼Œå³ 30ç§’/æ¬¡ã€‚
        # è®¾ç½®ä¸º 35 ç§’ä»¥ä¿ç•™å®‰å…¨ç¼“å†²ï¼Œé˜²æ­¢è§¦å‘ 429 é”™è¯¯ã€‚
        if i < deep_dive_count - 1: # æœ€åä¸€ç¯‡ä¸éœ€è¦ç­‰å¾…
            print("ç­‰å¾… 150 ç§’ä»¥ç¬¦åˆ API é€Ÿç‡é™åˆ¶...")
            time.sleep(150) 

    # 5. å†™å…¥æ–‡ä»¶
    with open("README.md", "w", encoding="utf-8") as f:
        f.write(md_content)
    print(f"æŠ¥å‘Šç”Ÿæˆå®Œæ¯•ï¼å·²ä¿å­˜è‡³ README.md")

if __name__ == "__main__":
    main()
