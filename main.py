import google.generativeai as genai
import requests
import os
import time
import re
import json
from datetime import datetime

# =================é…ç½®åŒºåŸŸ=================
# å»ºè®®æ£€æŸ¥ Key æ˜¯å¦å­˜åœ¨
GENAI_API_KEY = os.environ.get("GEMINI_API_KEY")
if not GENAI_API_KEY:
    raise ValueError("âŒ æœªæ£€æµ‹åˆ° GEMINI_API_KEYï¼Œè¯·æ£€æŸ¥ç¯å¢ƒå˜é‡è®¾ç½®")

genai.configure(api_key=GENAI_API_KEY)

MODEL_FAST = 'gemini-3.0-flash' 
MODEL_DEEP = 'gemini-3.0-flash' 

# æ ¸å¿ƒå…³æ³¨é¢†åŸŸ
CORE_KEYWORDS = ["Image Restoration", "Masked Autoregressive", "Flow Matching", "Super-Resolution", "Diffusion", "Image Generation"]

# =================æ•°æ®ç»“æ„=================
class Paper:
    def __init__(self, title, summary, url, source):
        self.title = title.replace('\n', ' ').strip()
        self.summary = summary.replace('\n', ' ').strip()
        self.url = url
        self.source = source
        self.score = 0
        self.reasoning = ""

# =================æŠ“å–æ¨¡å—=================
def get_huggingface_papers():
    print("ğŸ“¡ æ­£åœ¨æŠ“å– Hugging Face Daily Papers...")
    results = []
    try:
        url = "https://huggingface.co/api/daily_papers"
        response = requests.get(url, timeout=8)
        if response.status_code == 200:
            data = response.json()
            # HF API æœ‰æ—¶è¿”å›çš„æ˜¯ list æœ‰æ—¶æ˜¯æŒ‰æ—¥æœŸåˆ†ç±»çš„ dictï¼Œåšä¸ªå…¼å®¹
            items = data if isinstance(data, list) else []
            if not items and isinstance(data, dict):
                 # å°è¯•è·å–æœ€æ–°æ—¥æœŸçš„æ•°æ® (ç®€åŒ–é€»è¾‘)
                 items = list(data.values())[0] if data else []

            for item in items[:8]: # é™åˆ¶æ•°é‡ä»¥åŠ å¿«æµ‹è¯•
                paper_info = item['paper']
                results.append(Paper(
                    title=paper_info['title'],
                    summary=paper_info['summary'],
                    url=f"https://arxiv.org/abs/{paper_info['id']}",
                    source="HuggingFace ğŸ”¥"
                ))
    except Exception as e:
        print(f"âš ï¸ HF æŠ“å–é‡åˆ°é—®é¢˜: {e}")
    return results

def get_openreview_papers():
    print("ğŸ“¡ æ­£åœ¨æŠ“å– OpenReview (ICLR 2025)...")
    results = []
    try:
        # ä½¿ç”¨æ›´é€šç”¨çš„ API æœç´¢å…³é”®è¯ï¼Œé¿å… venueid å˜åŠ¨å¯¼è‡´æŠ“ç©º
        # è¿™é‡Œæ¼”ç¤ºæœç´¢ 'Image Restoration' ç›¸å…³çš„æœ€æ–°æŠ•ç¨¿
        domain = "ICLR.cc/2025/Conference" # æˆ– use search query
        api_url = f"https://api2.openreview.net/notes?content.venueid={domain}&limit=8"
        
        response = requests.get(api_url, timeout=8)
        if response.status_code == 200:
            notes = response.json().get('notes', [])
            for note in notes:
                content = note.get('content', {})
                # V2 API ç»“æ„å…¼å®¹
                title = content.get('title', {}).get('value', 'No Title')
                abstract = content.get('abstract', {}).get('value', 'No Abstract')
                results.append(Paper(
                    title=title,
                    summary=abstract,
                    url=f"https://openreview.net/forum?id={note.get('id')}",
                    source="OpenReview ğŸ“"
                ))
        else:
            print(f"OpenReview API çŠ¶æ€ç : {response.status_code}")
    except Exception as e:
        print(f"âš ï¸ OpenReview æŠ“å–é‡åˆ°é—®é¢˜: {e}")
    return results

# =================AI åˆ†ææ¨¡å—=================
def score_paper(paper):
    """
    ä½¿ç”¨ Gemini Flash æ‰“åˆ†
    ä¼˜åŒ–ç‚¹ï¼šä¼ å…¥äº† CORE_KEYWORDSï¼Œå¹¶å¼ºåˆ¶ JSON è¾“å‡ºä»¥ä¾¿è§£æ
    """
    model = genai.GenerativeModel(
        MODEL_FAST,
        generation_config={"response_mime_type": "application/json"} # å¼ºåˆ¶ JSON
    )
    
    # å°†å…³é”®è¯åˆ—è¡¨è½¬ä¸ºå­—ç¬¦ä¸²
    keywords_str = ", ".join(CORE_KEYWORDS)
    
    prompt = f"""
    You are a Senior Computer Vision Researcher acting as a paper filter.
    
    My Core Interests: [{keywords_str}].
    
    Task: Rate the following paper from 1 to 10 based on relevance to my interests and potential impact.
    - 10: Must read. Directly addresses core interests with high novelty.
    - 1: Irrelevant.
    
    Paper Title: {paper.title}
    Abstract: {paper.summary[:1000]} (truncated)
    
    Output strictly in JSON format:
    {{
        "score": int,
        "reason": "short explanation in Chinese"
    }}
    """
    try:
        response = model.generate_content(prompt)
        data = json.loads(response.text)
        return data.get("score", 0), data.get("reason", "è§£æå¤±è´¥")
    except Exception as e:
        print(f"âš ï¸ è¯„åˆ†å‡ºé”™ ({paper.title[:10]}...): {e}")
        return 0, "Error"

def deep_analyze(paper):
    print(f"ğŸ§  æ­£åœ¨æ·±åº¦é˜…è¯»: {paper.title}...")
    model = genai.GenerativeModel(MODEL_DEEP)
    prompt = f"""
    è¯·ä½œä¸ºè®¡ç®—æœºè§†è§‰ä¸“å®¶ï¼Œæ·±åº¦è§£æè¿™ç¯‡è®ºæ–‡ã€‚
    æ ¸å¿ƒå…³æ³¨ç‚¹ï¼š{", ".join(CORE_KEYWORDS)}
    
    è®ºæ–‡æ ‡é¢˜ï¼š{paper.title}
    æ‘˜è¦ï¼š{paper.summary}
    
    è¯·ç”¨ä¸­æ–‡ Markdown æ ¼å¼è¾“å‡ºï¼š
    1. **æ ¸å¿ƒåˆ›æ–°ç‚¹ (Key Contribution)**: ä¸€å¥è¯æ€»ç»“ã€‚
    2. **æŠ€æœ¯ç»†èŠ‚ (Methodology)**: å®ƒæ˜¯å¦‚ä½•ç»“åˆ {CORE_KEYWORDS[0]} æˆ–ç›¸å…³æŠ€æœ¯çš„ï¼Ÿ
    3. **å¯¹æˆ‘çš„å¯å‘ (Takeaway)**: é’ˆå¯¹åš Image Restoration çš„ç ”ç©¶å‘˜ï¼Œè¿™å°±è¯æœ‰ä»€ä¹ˆå€Ÿé‰´æ„ä¹‰ï¼Ÿ
    4. **æ½œåœ¨ç¼ºé™· (Limitations)**.
    """
    try:
        response = model.generate_content(prompt)
        return response.text
    except Exception as e:
        return f"æ·±åº¦åˆ†æå¤±è´¥: {e}"

# =================æŠ¥å‘Šç”Ÿæˆæ¨¡å—=================
def save_report(all_papers, top_data):
    """
    ç”Ÿæˆ Markdown æŠ¥å‘Šå¹¶å†™å…¥ README.md
    """
    print("\nğŸ“ æ­£åœ¨ç”Ÿæˆæ—¥æŠ¥æ–‡ä»¶...")
    
    # è·å–å½“å‰æ—¥æœŸ
    current_date = datetime.now().strftime("%Y-%m-%d")
    
    # æ„å»º Markdown å†…å®¹
    md_content = []
    md_content.append(f"# ğŸš€ CV è®ºæ–‡æ—¥æŠ¥ | {current_date}\n")
    md_content.append(f"> ğŸ¤– ä»Šæ—¥åŠ¨æ€ï¼šæ‰«æ {len(all_papers)} ç¯‡ï¼Œç²¾é€‰ {len(top_data)} ç¯‡æ·±åº¦è§£è¯»ã€‚\n")
    
    # ç›®å½•éƒ¨åˆ†
    md_content.append("## ğŸ“‹ ç›®å½• (Quick View)\n")
    if not top_data:
        md_content.append("ä»Šæ—¥æ— é«˜åˆ†æ¨èã€‚\n")
    else:
        for idx, item in enumerate(top_data):
            paper = item['paper']
            # åˆ›å»ºç®€å•çš„é”šç‚¹é“¾æ¥
            anchor = f"item-{idx}"
            md_content.append(f"- [{paper.title}](#{anchor}) (Score: {paper.score})\n")
    
    md_content.append("\n---\n")
    
    # æ·±åº¦è§£è¯»éƒ¨åˆ†
    md_content.append("## ğŸ§  æ·±åº¦è§£è¯» (Deep Dive)\n")
    if not top_data:
        md_content.append("æš‚æ—¶æ²¡æœ‰æ·±åº¦åˆ†æå†…å®¹ã€‚\n")
    else:
        for idx, item in enumerate(top_data):
            paper = item['paper']
            analysis = item['analysis']
            anchor = f"item-{idx}"
            
            md_content.append(f"### <a id='{anchor}'></a>{idx+1}. {paper.title}\n")
            md_content.append(f"**æ¥æº**: {paper.source} | **è¯„åˆ†**: {paper.score}/10\n")
            md_content.append(f"**åŸæ–‡é“¾æ¥**: [{paper.url}]({paper.url})\n\n")
            md_content.append(f"{analysis}\n")
            md_content.append("\n---\n")

    # å†™å…¥æ–‡ä»¶
    try:
        with open("README.md", "w", encoding="utf-8") as f:
            f.writelines(md_content)
        print("âœ… README.md æ›´æ–°æˆåŠŸï¼")
    except Exception as e:
        print(f"âŒ å†™å…¥æ–‡ä»¶å¤±è´¥: {e}")

# =================ä¸»ç¨‹åº=================
def main():
    # 1. æŠ“å–
    all_papers = get_huggingface_papers() + get_openreview_papers()
    print(f"ğŸ“š æ€»è®¡è·å–å€™é€‰è®ºæ–‡: {len(all_papers)} ç¯‡")
    
    if not all_papers:
        print("âŒ æœªè·å–åˆ°ä»»ä½•è®ºæ–‡ï¼Œè¯·æ£€æŸ¥ API æˆ–ç½‘ç»œã€‚")
        return

    # 2. å¿«é€Ÿæ‰“åˆ† (å»é™¤é•¿æ—¶é—´ sleepï¼ŒFlash å¾ˆå¿«ä¸”é™é¢é«˜)
    print("\nâš¡ å¼€å§‹ AI æé€Ÿç­›é€‰...")
    for i, p in enumerate(all_papers):
        # ç®€å•çš„è¿›åº¦æ˜¾ç¤º
        print(f"\rå¤„ç†ä¸­ [{i+1}/{len(all_papers)}]: {p.title[:30]}...", end="")
        p.score, p.reasoning = score_paper(p)

        time.sleep(20) 
    
    print("\nâœ… ç­›é€‰å®Œæˆï¼")

    # 3. æ’åºå¹¶å– Top 2
    # è¿‡æ»¤æ‰ä½åˆ† (ä¾‹å¦‚ 5 åˆ†ä»¥ä¸‹)ï¼Œç„¶åæ’åº
    top_candidates = [p for p in all_papers if p.score >= 5]
    top_2 = sorted(top_candidates, key=lambda x: x.score, reverse=True)[:2]
    
    if not top_2:
        print("ğŸ˜… æ²¡æœ‰æ‰¾åˆ°é«˜åˆ†è®ºæ–‡ï¼Œå¯èƒ½æ˜¯ä»Šå¤©çš„è®ºæ–‡éƒ½ä¸å…³æ³¨ç‚¹æ— å…³ã€‚")
        # å…œåº•ï¼šå–åŸå§‹æœ€é«˜åˆ†
        top_2 = sorted(all_papers, key=lambda x: x.score, reverse=True)[:2]

    # 4. è¾“å‡ºç»“æœå¹¶æ”¶é›†æ•°æ®ç”¨äºæŠ¥å‘Š
    print("\n" + "="*50)
    print(f"ğŸš€ ä»Šæ—¥é¡¶çº§æ¨è (TOP 2)")
    print("="*50 + "\n")
    
    report_data = [] # ç”¨äºå­˜å‚¨ç”Ÿæˆçš„æŠ¥å‘Šå†…å®¹

    for i, p in enumerate(top_2):
        print(f"ğŸ† ç¬¬ {i+1} åï¼š{p.title}")
        print(f"æ¥æº: {p.source} | ğŸ’¡ è¯„åˆ†: {p.score}/10")
        print(f"ç†ç”±: {p.reasoning}")
        print(f"é“¾æ¥: {p.url}")
        print("-" * 30)
        
        # æ·±åº¦åˆ†æ
        analysis = deep_analyze(p)
        print(f"\n{analysis}\n")
        print("="*50 + "\n")
        
        # æ”¶é›†æ•°æ®
        report_data.append({
            "paper": p,
            "analysis": analysis
        })

        # Pro æ¨¡å‹ç¨å¾®å¤šæ­‡ä¸€ä¼š
        time.sleep(100)
    
    # 5. ç”Ÿæˆå¹¶ä¿å­˜ Markdown æŠ¥å‘Š
    save_report(all_papers, report_data)

if __name__ == "__main__":
    main()
