import google.generativeai as genai
import requests
import os
import time
import re
import json
from datetime import datetime

# =================é…ç½®åŒºåŸŸ=================
# å»ºè®®æ£€æŸ¥ Key æ˜¯å¦å­˜åœ¨
GENAI_API_KEY = os.environ.get("GEMINI_API_KEY")
if not GENAI_API_KEY:
    raise ValueError("âŒ æœªæ£€æµ‹åˆ° GEMINI_API_KEYï¼Œè¯·æ£€æŸ¥ç¯å¢ƒå˜é‡è®¾ç½®")

genai.configure(api_key=GENAI_API_KEY)

MODEL_FAST = 'gemini-2.5-flash' 
MODEL_DEEP = 'gemini-2.5-flash' 

# æ ¸å¿ƒå…³æ³¨é¢†åŸŸ
CORE_KEYWORDS = ["Image Restoration", "Masked Autoregressive", "Flow Matching", "Super-Resolution", "Diffusion", "Image Generation"]

# =================æ•°æ®ç»“æ„=================
class Paper:
    def __init__(self, title, summary, url, source):
        self.title = title.replace('\n', ' ').strip()
        self.summary = summary.replace('\n', ' ').strip()
        self.url = url
        self.source = source
        self.score = 0
        self.reasoning = ""

# =================æŠ“å–æ¨¡å—=================
def get_huggingface_papers():
    print("ğŸ“¡ æ­£åœ¨æŠ“å– Hugging Face Daily Papers (Top 15)...")
    results = []
    try:
        url = "https://huggingface.co/api/daily_papers"
        response = requests.get(url, timeout=10)
        if response.status_code == 200:
            data = response.json()
            # HF API æœ‰æ—¶è¿”å›çš„æ˜¯ list æœ‰æ—¶æ˜¯æŒ‰æ—¥æœŸåˆ†ç±»çš„ dictï¼Œåšä¸ªå…¼å®¹
            items = data if isinstance(data, list) else []
            if not items and isinstance(data, dict):
                 # å°è¯•è·å–æœ€æ–°æ—¥æœŸçš„æ•°æ® (ç®€åŒ–é€»è¾‘)
                 items = list(data.values())[0] if data else []

            # ä¿®æ”¹ç‚¹ï¼šé™åˆ¶æ•°é‡å¢åŠ è‡³ 15
            for item in items[:15]: 
                paper_info = item['paper']
                results.append(Paper(
                    title=paper_info['title'],
                    summary=paper_info['summary'],
                    url=f"https://arxiv.org/abs/{paper_info['id']}",
                    source="HuggingFace ğŸ”¥"
                ))
    except Exception as e:
        print(f"âš ï¸ HF æŠ“å–é‡åˆ°é—®é¢˜: {e}")
    return results

# =================AI åˆ†ææ¨¡å—=================
def score_paper(paper):
    """
    ä½¿ç”¨ Gemini Flash æ‰“åˆ†
    ä¿®æ”¹ç‚¹ï¼šåˆ†å€¼æ”¹ä¸º 0-100ï¼Œè§’è‰²æ”¹ä¸ºä¸¥æ ¼å®¡ç¨¿äºº
    """
    model = genai.GenerativeModel(
        MODEL_FAST,
        generation_config={"response_mime_type": "application/json"} # å¼ºåˆ¶ JSON
    )
    
    # å°†å…³é”®è¯åˆ—è¡¨è½¬ä¸ºå­—ç¬¦ä¸²
    keywords_str = ", ".join(CORE_KEYWORDS)
    
    prompt = f"""
    You are a strict Reviewer for a top-tier Computer Vision Conference (e.g., CVPR, ICCV, ECCV).
    
    My Research Interests: [{keywords_str}].
    
    Task: Rate the following paper strictly from 0 to 100 based on its scientific value, novelty, and relevance to my interests.
    
    Scoring Criteria:
    - 90-100: Strong Accept. Groundbreaking work, highly relevant, must read.
    - 75-89: Accept. Solid work with good relevance.
    - 60-74: Weak Accept / Borderline. Some flaws or weak relevance, but has merit.
    - < 60: Reject. Irrelevant, lacks novelty, or poor quality.
    
    Paper Title: {paper.title}
    Abstract: {paper.summary[:1500]} (truncated)
    
    Output strictly in JSON format:
    {{
        "score": int,
        "reason": "One sentence critique in Chinese, explaining the score."
    }}
    """
    try:
        response = model.generate_content(prompt)
        data = json.loads(response.text)
        return data.get("score", 0), data.get("reason", "è§£æå¤±è´¥")
    except Exception as e:
        print(f"âš ï¸ è¯„åˆ†å‡ºé”™ ({paper.title[:10]}...): {e}")
        return 0, "Error"

def deep_analyze(paper):
    print(f"ğŸ§  æ­£åœ¨æ·±åº¦é˜…è¯»: {paper.title}...")
    model = genai.GenerativeModel(MODEL_DEEP)
    prompt = f"""
    è¯·ä½œä¸ºè®¡ç®—æœºè§†è§‰ä¸“å®¶ï¼Œæ·±åº¦è§£æè¿™ç¯‡è®ºæ–‡ã€‚
    æ ¸å¿ƒå…³æ³¨ç‚¹ï¼š{", ".join(CORE_KEYWORDS)}
    
    è®ºæ–‡æ ‡é¢˜ï¼š{paper.title}
    æ‘˜è¦ï¼š{paper.summary}
    
    è¯·ç”¨ä¸­æ–‡ Markdown æ ¼å¼è¾“å‡ºï¼š
    1. **æ ¸å¿ƒåˆ›æ–°ç‚¹ (Key Contribution)**: ä¸€å¥è¯æ€»ç»“ã€‚
    2. **æŠ€æœ¯ç»†èŠ‚ (Methodology)**: å®ƒæ˜¯å¦‚ä½•ç»“åˆ {CORE_KEYWORDS[0]} æˆ–ç›¸å…³æŠ€æœ¯çš„ï¼Ÿ
    3. **å¯¹æˆ‘çš„å¯å‘ (Takeaway)**: é’ˆå¯¹åš Image Restoration çš„ç ”ç©¶å‘˜ï¼Œè¿™å°±è¯æœ‰ä»€ä¹ˆå€Ÿé‰´æ„ä¹‰ï¼Ÿ
    4. **æ½œåœ¨ç¼ºé™· (Limitations)**.
    """
    try:
        response = model.generate_content(prompt)
        return response.text
    except Exception as e:
        return f"æ·±åº¦åˆ†æå¤±è´¥: {e}"

# =================æŠ¥å‘Šç”Ÿæˆæ¨¡å—=================
def save_report(all_papers, top_data):
    """
    ç”Ÿæˆ Markdown æŠ¥å‘Šå¹¶å†™å…¥ README.md
    """
    print("\nğŸ“ æ­£åœ¨ç”Ÿæˆæ—¥æŠ¥æ–‡ä»¶...")
    
    # è·å–å½“å‰æ—¥æœŸ
    current_date = datetime.now().strftime("%Y-%m-%d")
    
    # æ„å»º Markdown å†…å®¹
    md_content = []
    md_content.append(f"# ğŸš€ CV è®ºæ–‡æ—¥æŠ¥ | {current_date}\n")
    md_content.append(f"> ğŸ¤– ä»Šæ—¥åŠ¨æ€ï¼šæ‰«æ {len(all_papers)} ç¯‡ (HF Top 15)ï¼Œç²¾é€‰ {len(top_data)} ç¯‡æ·±åº¦è§£è¯»ã€‚\n")
    
    # ç›®å½•éƒ¨åˆ†
    md_content.append("## ğŸ“‹ ç›®å½• (Quick View)\n")
    if not top_data:
        md_content.append("ä»Šæ—¥æ— ç¬¦åˆæ ‡å‡†ï¼ˆScore >= 60ï¼‰çš„é«˜åˆ†æ¨èã€‚\n")
    else:
        for idx, item in enumerate(top_data):
            paper = item['paper']
            # åˆ›å»ºç®€å•çš„é”šç‚¹é“¾æ¥
            anchor = f"item-{idx}"
            md_content.append(f"- [{paper.title}](#{anchor}) (Score: {paper.score})\n")
    
    md_content.append("\n---\n")
    
    # æ·±åº¦è§£è¯»éƒ¨åˆ†
    md_content.append("## ğŸ§  æ·±åº¦è§£è¯» (Deep Dive)\n")
    if not top_data:
        md_content.append("æš‚æ—¶æ²¡æœ‰æ·±åº¦åˆ†æå†…å®¹ã€‚\n")
    else:
        for idx, item in enumerate(top_data):
            paper = item['paper']
            analysis = item['analysis']
            anchor = f"item-{idx}"
            
            md_content.append(f"### <a id='{anchor}'></a>{idx+1}. {paper.title}\n")
            md_content.append(f"**æ¥æº**: {paper.source} | **è¯„åˆ†**: {paper.score}/100\n")
            md_content.append(f"**åŸæ–‡é“¾æ¥**: [{paper.url}]({paper.url})\n\n")
            md_content.append(f"{analysis}\n")
            md_content.append("\n---\n")

    # å†™å…¥æ–‡ä»¶
    try:
        with open("README.md", "w", encoding="utf-8") as f:
            f.writelines(md_content)
        print("âœ… README.md æ›´æ–°æˆåŠŸï¼")
    except Exception as e:
        print(f"âŒ å†™å…¥æ–‡ä»¶å¤±è´¥: {e}")

# =================ä¸»ç¨‹åº=================
def main():
    # 1. æŠ“å– (ä»…ä¿ç•™ HuggingFace)
    all_papers = get_huggingface_papers()
    print(f"ğŸ“š æ€»è®¡è·å–å€™é€‰è®ºæ–‡: {len(all_papers)} ç¯‡")
    
    if not all_papers:
        print("âŒ æœªè·å–åˆ°ä»»ä½•è®ºæ–‡ï¼Œè¯·æ£€æŸ¥ API æˆ–ç½‘ç»œã€‚")
        return

    # 2. å¿«é€Ÿæ‰“åˆ†
    print("\nâš¡ å¼€å§‹ AI æé€Ÿä¸¥æ ¼ç­›é€‰ (Strict Mode)...")
    for i, p in enumerate(all_papers):
        # ç®€å•çš„è¿›åº¦æ˜¾ç¤º
        print(f"\rå¤„ç†ä¸­ [{i+1}/{len(all_papers)}]: {p.title[:30]}...", end="")
        p.score, p.reasoning = score_paper(p)
        # Flashæ¨¡å‹é€Ÿåº¦å¾ˆå¿«ï¼Œä¿ç•™å°‘é‡é—´éš”é˜²æ­¢è§¦å‘ç¬æ—¶é£æ§
        time.sleep(2) 
    
    print("\nâœ… ç­›é€‰å®Œæˆï¼")

    # 3. æ’åºå¹¶å– Top 2
    # ä¿®æ”¹ç‚¹ï¼šä¸¥æ ¼è¿‡æ»¤æ‰ä½äº 60 åˆ†çš„è®ºæ–‡
    top_candidates = [p for p in all_papers if p.score >= 60]
    
    # æŒ‰åˆ†æ•°é™åºæ’åˆ—
    top_candidates = sorted(top_candidates, key=lambda x: x.score, reverse=True)
    
    # å–å‰ 2 å
    top_2 = top_candidates[:2]
    
    if not top_2:
        print("ğŸ˜… ä»Šæ—¥æ— è®ºæ–‡è¾¾åˆ° 60 åˆ†åŠæ ¼çº¿ï¼Œå…¨éƒ¨ä¸¢å¼ƒã€‚")
    
    # 4. è¾“å‡ºç»“æœå¹¶æ”¶é›†æ•°æ®ç”¨äºæŠ¥å‘Š
    print("\n" + "="*50)
    print(f"ğŸš€ ä»Šæ—¥é¡¶çº§æ¨è (TOP {len(top_2)})")
    print("="*50 + "\n")
    
    report_data = [] # ç”¨äºå­˜å‚¨ç”Ÿæˆçš„æŠ¥å‘Šå†…å®¹

    for i, p in enumerate(top_2):
        print(f"ğŸ† ç¬¬ {i+1} åï¼š{p.title}")
        print(f"æ¥æº: {p.source} | ğŸ’¡ è¯„åˆ†: {p.score}/100")
        print(f"ç†ç”±: {p.reasoning}")
        print(f"é“¾æ¥: {p.url}")
        print("-" * 30)
        
        # æ·±åº¦åˆ†æ
        analysis = deep_analyze(p)
        print(f"\n{analysis}\n")
        print("="*50 + "\n")
        
        # æ”¶é›†æ•°æ®
        report_data.append({
            "paper": p,
            "analysis": analysis
        })

        # Pro æ¨¡å‹ç¨å¾®å¤šæ­‡ä¸€ä¼š
        time.sleep(30)
    
    # 5. ç”Ÿæˆå¹¶ä¿å­˜ Markdown æŠ¥å‘Š
    save_report(all_papers, report_data)

if __name__ == "__main__":
    main()
