import arxiv
import google.generativeai as genai
import requests
import os
import datetime
import time
import re

# =================é…ç½®åŒºåŸŸ=================
# API é…ç½®
GENAI_API_KEY = os.environ.get("GEMINI_API_KEY")
genai.configure(api_key=GENAI_API_KEY)

# æ¨¡å‹é€‰æ‹©
# å¼ºçƒˆå»ºè®®ä½¿ç”¨ 'gemini-1.5-flash'ï¼Œè¿™æ˜¯ç›®å‰ Free Tier æœ€ç¨³å®šã€é¢åº¦æœ€é«˜çš„æ¨¡å‹ã€‚
# å¦‚æœä½ æœ‰ gemini-3 çš„æƒé™ï¼Œå¯ä»¥æ”¹å› 'gemini-3-flash-preview'
MODEL_FAST = 'gemini-3-flash-preview' 
MODEL_DEEP = 'gemini-2.5-pro' 

# æ ¸å¿ƒå…³é”®è¯ (å‘½ä¸­è¿™äº›è¯çš„è®ºæ–‡å°†ä¼˜å…ˆå¤„ç†)
CORE_KEYWORDS = [
    "Image Restoration", "Super-Resolution", "Denoising", "Flow Based",
    "Masked Autoregressive", "Diffusion Model", "Generative Prior",
    "High-Fidelity", "Perceptual Quality"
]

# å¹¿æ³›å…³é”®è¯ (ç”¨äºä¿ç•™å€™é€‰)
BROAD_KEYWORDS = [
    "Computer Vision", "Generative", "Transformer", "Gaussian Splatting", 
    "NeRF", "3D Generation", "Video Synthesis", "Multimodal"
]

# æ’é™¤å…³é”®è¯ (è¿‡æ»¤æ— å…³é¢†åŸŸ)
EXCLUDE_KEYWORDS = ["Medical", "MRI", "CT Scan"]

# =================æ•°æ®ç»“æ„=================
class Paper:
    def __init__(self, arxiv_id, title, summary, url, source="arXiv"):
        self.id = arxiv_id
        self.title = title.replace('\n', ' ')
        self.summary = summary.replace('\n', ' ')
        self.url = url
        self.source = source # "HuggingFace" or "arXiv"
        self.score = 0
        self.reasoning = ""
        self.analysis = ""

# =================æŠ“å–æ¨¡å—=================
def get_huggingface_papers():
    """è·å– HF Daily Papers (é«˜è´¨é‡æº)"""
    print("æ­£åœ¨æŠ“å– Hugging Face Daily Papers...")
    papers = {}
    try:
        url = "https://huggingface.co/api/daily_papers"
        response = requests.get(url, timeout=10)
        if response.status_code == 200:
            data = response.json()
            for item in data[:10]: # åªå–å‰10çƒ­åº¦
                aid = item['paper']['id']
                papers[aid] = "HuggingFace Hot"
    except Exception as e:
        print(f"HF æŠ“å–å¤±è´¥ (éè‡´å‘½é”™è¯¯): {e}")
    return papers

def fetch_papers_data(hf_ids):
    """ä¸»æŠ“å–é€»è¾‘ï¼šåˆå¹¶ HF å’Œ arXiv æ•°æ®"""
    client = arxiv.Client()
    
    print("æ­£åœ¨æœç´¢ arXiv æœ€æ–°è®ºæ–‡ (Max: 40)...")
    search_arxiv = arxiv.Search(
        query="cat:cs.CV",
        max_results=40, # ä¿æŒ 40 ç¯‡ä»¥èŠ‚çœé¢åº¦
        sort_by=arxiv.SortCriterion.SubmittedDate
    )
    
    results = []
    seen_ids = set()

    # å¤„ç† arXiv ç»“æœ
    for result in client.results(search_arxiv):
        aid = result.get_short_id().split('v')[0]
        seen_ids.add(aid)
        
        source = "arXiv Latest"
        if aid in hf_ids:
            source = "ğŸ”¥ HuggingFace Hot"
        
        p = Paper(aid, result.title, result.summary, result.entry_id, source)
        results.append(p)

    # è¡¥å……æŠ“å– HF é—æ¼çš„è®ºæ–‡
    missing_ids = [hid for hid in hf_ids if hid not in seen_ids]
    if missing_ids:
        print(f"è¡¥å……æŠ“å– {len(missing_ids)} ç¯‡ HF çƒ­é—¨è®ºæ–‡...")
        try:
            search_missing = arxiv.Search(id_list=missing_ids)
            for result in client.results(search_missing):
                aid = result.get_short_id().split('v')[0]
                p = Paper(aid, result.title, result.summary, result.entry_id, "ğŸ”¥ HuggingFace Hot")
                results.append(p)
        except Exception as e:
            print(f"è¡¥å……æŠ“å–å¤±è´¥: {e}")
            
    return results

# =================AI åˆ†ææ¨¡å—=================
def filter_and_score(papers):
    """
    ç¬¬ä¸€å±‚ï¼šPython å…³é”®è¯ç¡¬è¿‡æ»¤
    ç¬¬äºŒå±‚ï¼šGemini Flash å¿«é€Ÿè¯„åˆ†
    """
    candidates = []
    
    # 1. ç¡¬è¿‡æ»¤
    for p in papers:
        text = (p.title + p.summary).lower()
        if any(ex.lower() in text for ex in EXCLUDE_KEYWORDS):
            continue
        
        if any(k.lower() in text for k in (CORE_KEYWORDS + BROAD_KEYWORDS)) or "Hot" in p.source:
            candidates.append(p)
            
    print(f"åˆç­›é€šè¿‡: {len(candidates)} ç¯‡ï¼Œå¼€å§‹ AI è¯„åˆ†...")
    
    # 2. AI è¯„åˆ†
    model = genai.GenerativeModel(MODEL_FAST)
    
    scored_papers = []
    for i, p in enumerate(candidates):
        print(f"æ­£åœ¨è¯„åˆ† ({i+1}/{len(candidates)}): {p.title[:30]}...")

        base_priority = "High" if any(k.lower() in p.title.lower() for k in CORE_KEYWORDS) else "Normal"
        
        prompt = f"""
        Role: CV Research Assistant.
        Task: Rate relevance (1-10) for a researcher focusing on: Image Restoration, MAR, Super-Resolution.
        Input:
        Title: {p.title}
        Abstract: {p.summary}
        
        Output format strictly: Score | One-sentence reason
        Example: 8 | Proposes a novel MAR variant for deblurring.
        """
        try:
            response = model.generate_content(prompt)
            text = response.text.strip()
            score_str = text.split('|')[0].strip()
            
            # è§£æåˆ†æ•°
            found_scores = re.findall(r"\d+", score_str)
            if found_scores:
                p.score = int(float(found_scores[0]))
            else:
                p.score = 5 
            
            p.reasoning = text.split('|')[1].strip() if '|' in text else text
            
            if base_priority == "High" and p.score < 7:
                p.score = 7 
            
            if p.score > 6:
                scored_papers.append(p)
                print(f"  -> è¯„åˆ†æˆåŠŸ: {p.score}")

        except Exception as e:
            # æ•è·é”™è¯¯ä½†ä¸ä¸­æ–­å¾ªç¯
            print(f"  -> è¯„åˆ†å¤±è´¥: {e}")
            # ã€å…³é”®ä¿®æ”¹ã€‘è¿™é‡Œå»æ‰äº† continueï¼Œç¡®ä¿æ— è®ºæˆåŠŸå¤±è´¥éƒ½ä¼šæ‰§è¡Œä¸‹é¢çš„ sleep
        
        # ã€å…³é”®ä¿®æ”¹ã€‘å¼ºåˆ¶ç­‰å¾… 60 ç§’
        # æ”¾åœ¨ try...except å¤–é¢ï¼Œç¡®ä¿æ¯æ¬¡å¾ªç¯æœ«å°¾éƒ½æ‰§è¡Œ
        print("  -> å†·å´ 60 ç§’ä»¥ä¿æŠ¤ API é…é¢...")
        time.sleep(60) 

    scored_papers.sort(key=lambda x: x.score, reverse=True)
    return scored_papers

def deep_analyze_paper(paper):
    """ä½¿ç”¨æ¨¡å‹è¿›è¡Œæ·±åº¦å®¡ç¨¿"""
    model = genai.GenerativeModel(MODEL_DEEP)
    
    prompt = f"""
    You are an expert reviewer for ECCV/CVPR.
    Analyze the following paper strictly in CHINESE (Markdown).
    
    Target Audience: A researcher working on **Image Restoration** and **Masked Autoregressive (MAR)** models.
    
    Paper:
    Title: {paper.title}
    Abstract: {paper.summary}
    
    Please provide:
    1. **æ ¸å¿ƒåˆ›æ–°ç‚¹ (The "Hook")**: What is strictly new? (1-2 bullet points)
    2. **æ–¹æ³•è®ºæ‹†è§£ (Methodology)**: How does it work? 
       - If it mentions MAR or Transformers, compare it with standard approaches.
    3. **æ½œåœ¨ç¼ºé™·/å±€é™ (Critical Review)**: As a reviewer, what would you challenge? (e.g., complexity, lack of specific baselines)
    4. **å¯¹æˆ‘çš„å¯å‘**: How can this apply to Image Restoration tasks?
    
    Output strictly in Markdown. No preamble.
    """
    
    try:
        response = model.generate_content(prompt)
        return response.text
    except Exception as e:
        print(f"æ·±åº¦åˆ†æå¤±è´¥: {e}")
        return f"> **åˆ†æç”Ÿæˆå¤±è´¥**: API è°ƒç”¨å‡ºé”™ ({str(e)[:100]}...)"

# =================ä¸»ç¨‹åº=================
def main():
    # 1. è·å– ID åˆ—è¡¨
    hf_ids = get_huggingface_papers()
    
    # 2. æŠ“å–å…¨æ–‡æ•°æ®
    all_papers = fetch_papers_data(hf_ids)
    print(f"å…±æŠ“å–åŸå§‹è®ºæ–‡: {len(all_papers)} ç¯‡")
    
    if not all_papers:
        print("æœªæŠ“å–åˆ°è®ºæ–‡ï¼Œç¨‹åºç»“æŸã€‚")
        return

    # 3. ç­›é€‰ä¸è¯„åˆ†
    top_papers = filter_and_score(all_papers)
    print(f"æœ€ç»ˆå…¥é€‰ç²¾è¯»: {len(top_papers)} ç¯‡")
    
    # 4. ç”ŸæˆæŠ¥å‘Š
    today = datetime.date.today().strftime("%Y-%m-%d")
    md_content = f"# ğŸš€ CV è®ºæ–‡æ—¥æŠ¥ | {today}\n\n"
    md_content += f"> ğŸ¤– ä»Šæ—¥åŠ¨æ€ï¼šæ‰«æ {len(all_papers)} ç¯‡ï¼Œç²¾é€‰ {min(5, len(top_papers))} ç¯‡æ·±åº¦è§£è¯»ã€‚\n\n"
    
    # ç›®å½•éƒ¨åˆ†
    md_content += "## ğŸ“‹ ç›®å½• (Quick View)\n"
    for p in top_papers[:10]:
        icon = "ğŸ”¥" if "Hot" in p.source else "ğŸ“„"
        md_content += f"- **{p.score}åˆ†** {icon} [{p.title}]({p.url}) - *{p.reasoning}*\n"
    md_content += "\n---\n"
    
    # æ·±åº¦åˆ†æéƒ¨åˆ† (åªå–å‰ 5 ç¯‡)
    md_content += "## ğŸ§  æ·±åº¦è§£è¯» (Deep Dive)\n"
    
    deep_dive_count = min(5, len(top_papers))
    
    for i, p in enumerate(top_papers[:deep_dive_count]):
        print(f"æ­£åœ¨æ·±åº¦åˆ†æç¬¬ {i+1}/{deep_dive_count} ç¯‡: {p.title}...")
        analysis = deep_analyze_paper(p)
        
        md_content += f"### {i+1}. {p.title}\n"
        md_content += f"**æ¥æº**: {p.source} | **è¯„åˆ†**: {p.score}/10 | [Paper Link]({p.url})\n\n"
        md_content += f"{analysis}\n\n"
        md_content += "---\n"
        
        # æ·±åº¦åˆ†æé—´éš”
        if i < deep_dive_count - 1:
            print("ç­‰å¾… 300 ç§’ä»¥ç¬¦åˆ API é€Ÿç‡é™åˆ¶...")
            time.sleep(300) 

    # 5. å†™å…¥æ–‡ä»¶
    with open("README.md", "w", encoding="utf-8") as f:
        f.write(md_content)
    print(f"æŠ¥å‘Šç”Ÿæˆå®Œæ¯•ï¼å·²ä¿å­˜è‡³ README.md")

if __name__ == "__main__":
    main()
