import google.generativeai as genai
import requests
import os
import datetime
import time
import re

# =================é…ç½®åŒºåŸŸ=================
GENAI_API_KEY = os.environ.get("GEMINI_API_KEY")
genai.configure(api_key=GENAI_API_KEY)

# æ¨èä½¿ç”¨ Flash æ¨¡å‹è¿›è¡Œå¿«é€Ÿè¯„åˆ†ï¼ŒPro æ¨¡å‹è¿›è¡Œæ·±åº¦åˆ†æ
MODEL_FAST = 'gemini-1.5-flash' 
MODEL_DEEP = 'gemini-1.5-pro' 

# æ ¸å¿ƒå…³æ³¨é¢†åŸŸï¼ˆå½±å“è¯„åˆ†æƒé‡ï¼‰
CORE_KEYWORDS = ["Image Restoration", "Masked Autoregressive", "Flow Matching", "Super-Resolution", "Diffusion", "Image Generation"]

# =================æ•°æ®ç»“æ„=================
class Paper:
    def __init__(self, title, summary, url, source):
        self.title = title.replace('\n', ' ')
        self.summary = summary.replace('\n', ' ')
        self.url = url
        self.source = source
        self.score = 0
        self.reasoning = ""

# =================æŠ“å–æ¨¡å—=================
def get_huggingface_papers():
    """è·å– Hugging Face å‰ 10 ç¯‡çƒ­é—¨è®ºæ–‡"""
    print("æ­£åœ¨æŠ“å– Hugging Face Daily Papers...")
    results = []
    try:
        url = "https://huggingface.co/api/daily_papers"
        response = requests.get(url, timeout=10)
        if response.status_code == 200:
            data = response.json()
            for item in data[:10]: # ä»…å–å‰ 10
                paper_info = item['paper']
                aid = paper_info['id']
                results.append(Paper(
                    title=paper_info['title'],
                    summary=paper_info['summary'],
                    url=f"https://arxiv.org/abs/{aid}",
                    source="HuggingFace ğŸ”¥"
                ))
    except Exception as e:
        print(f"HF æŠ“å–å¤±è´¥: {e}")
    return results

def get_openreview_papers():
    """è·å– OpenReview æœ€æ–°æŠ•ç¨¿ (ä»¥æœ€è¿‘çš„å¤§ä¼šä¸ºä¾‹)"""
    print("æ­£åœ¨æŠ“å– OpenReview æœ€æ–°æŠ•ç¨¿...")
    results = []
    try:
        # æŠ“å– ICLR 2025 çš„æäº¤ä½œä¸ºç¤ºä¾‹ï¼ŒOpenReview API v2
        # æ³¨æ„ï¼švenue id ä¼šéšèµ›å­£å˜åŒ–
        api_url = "https://api2.openreview.net/notes?content.venueid=ICLR.cc/2025/Conference&limit=10"
        response = requests.get(api_url, timeout=10)
        if response.status_code == 200:
            notes = response.json().get('notes', [])
            for note in notes:
                content = note.get('content', {})
                title = content.get('title', {}).get('value', 'No Title')
                abstract = content.get('abstract', {}).get('value', 'No Abstract')
                note_id = note.get('id')
                results.append(Paper(
                    title=title,
                    summary=abstract,
                    url=f"https://openreview.net/forum?id={note_id}",
                    source="OpenReview ğŸ“"
                ))
    except Exception as e:
        print(f"OpenReview æŠ“å–å¤±è´¥: {e}")
    return results

# =================AI åˆ†ææ¨¡å—=================
def score_paper(paper):
    """ä½¿ç”¨ Gemini å¯¹è®ºæ–‡è¿›è¡Œ 1-10 åˆ†æ‰“åˆ†"""
    model = genai.GenerativeModel(MODEL_FAST)
    prompt = f"""
    Role: Senior CV Researcher.
    Task: Rate the importance (1-10) of this paper for someone working on Image Restoration and Masked Autoregressive (MAR) models.
    Paper Title: {paper.title}
    Abstract: {paper.summary}
    
    Output format: Score | One-sentence reason in Chinese.
    """
    try:
        response = model.generate_content(prompt)
        text = response.text.strip()
        score_match = re.search(r"(\d+)", text)
        score = int(score_match.group(1)) if score_match else 5
        reason = text.split('|')[1].strip() if '|' in text else text
        return score, reason
    except Exception as e:
        print(f"è¯„åˆ†å‡ºé”™: {e}")
        return 0, "Error"

def deep_analyze(paper):
    """å¯¹ Top 2 è®ºæ–‡è¿›è¡Œæ·±åº¦ä¸­æ–‡è§£è¯»"""
    model = genai.GenerativeModel(MODEL_DEEP)
    prompt = f"""
    è¯·ä½œä¸ºè®¡ç®—æœºè§†è§‰ä¸“å®¶ï¼Œæ·±åº¦è§£æè¿™ç¯‡è®ºæ–‡ï¼Œå¹¶ç”¨ä¸­æ–‡è¾“å‡ºï¼š
    1. æ ¸å¿ƒåˆ›æ–°ç‚¹ï¼š
    2. å¯¹å›¾åƒæ¢å¤(Image Restoration)ä»»åŠ¡çš„å¯å‘ï¼š
    3. æ½œåœ¨çš„å±€é™æ€§ï¼š
    
    è®ºæ–‡æ ‡é¢˜ï¼š{paper.title}
    æ‘˜è¦ï¼š{paper.summary}
    """
    try:
        response = model.generate_content(prompt)
        return response.text
    except Exception as e:
        return f"æ·±åº¦åˆ†æå¤±è´¥: {e}"

# =================ä¸»ç¨‹åº=================
def main():
    # 1. æŠ“å–æ•°æ®
    all_papers = get_huggingface_papers() + get_openreview_papers()
    print(f"æ€»è®¡æŠ“å–å€™é€‰è®ºæ–‡: {len(all_papers)} ç¯‡")
    
    # 2. ä¾æ¬¡æ‰“åˆ†ï¼ˆå¸¦å†·å´é˜²æ­¢ 429ï¼‰
    print("å¼€å§‹è¿›è¡Œ AI ç­›é€‰ä¸æ‰“åˆ†...")
    for i, p in enumerate(all_papers):
        p.score, p.reasoning = score_paper(p)
        print(f"[{i+1}/{len(all_papers)}] {p.score}åˆ† - {p.title[:40]}...")
        time.sleep(10) # è¯„åˆ†é˜¶æ®µæ¯ç¯‡é—´éš” 10 ç§’
        
    # 3. æ’åºå¹¶å– Top 2
    top_2 = sorted(all_papers, key=lambda x: x.score, reverse=True)[:2]
    
    # 4. è¾“å‡ºæœ€ç»ˆç»“æœ
    print("\n" + "="*50)
    print(f"ğŸš€ ä»Šæ—¥é¡¶çº§æ¨è (TOP 2)")
    print("="*50 + "\n")
    
    for i, p in enumerate(top_2):
        print(f"ç¬¬ {i+1} ç¯‡ï¼š{p.title}")
        print(f"æ¥æº: {p.source} | è¯„åˆ†: {p.score}/10")
        print(f"é“¾æ¥: {p.url}")
        print("-" * 20)
        
        # æ·±åº¦åˆ†æéœ€è¦è¾ƒå¤š Tokenï¼Œå†æ¬¡ç­‰å¾…ç¡®ä¿ API ç¨³å®š
        time.sleep(30)
        analysis = deep_analyze(p)
        print(f"ã€æ·±åº¦è§£è¯»ã€‘\n{analysis}\n")
        print("="*50 + "\n")

if __name__ == "__main__":
    main()
